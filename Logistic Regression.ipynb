{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression to perform two-class and multi-class classification for real-world tasks. \n",
    "### Part-1 (Binary Classification):\n",
    "\n",
    "For this problem, we will use a subset of [Wisconsin Breast cancer dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic). Note that this dataset has some information missing. \n",
    "\n",
    "**1.1 Data Munging** \n",
    "\n",
    "Cleaning the data is essential when dealing with real world problems. Training and testing data is stored in \"data/wisconsin_data\" folder. You have to perform the following:\n",
    "* Read the training and testing data. Print the number of features in the dataset.\n",
    "* For the data label, print the total number of 1's and 0's in the training and testing data. Comment on the class distribution. Is it balanced or unbalanced?\n",
    "* Print the number of features with missing entries.\n",
    "* Fill the missing entries. For filling any feature, you can use either mean or median value of the feature values from observed entries.\n",
    "* Normalize the training and testing data.\n",
    "\n",
    "**1.2 Logistic Regression ** \n",
    "Train logistic regression models with L1 regularization and L2 regularization using alpha = 0.1 and lambda = 0.1. Report accuracy, precision, recall, f1-score and print the confusion matrix.\n",
    "\n",
    "**1.3 Choosing the best hyper-parameter ** \n",
    "* For L1 model, choose the best alpha value from the following set: {0.1,1,3,10,33,100,333,1000, 3333, 10000, 33333}.\n",
    "* For L2 model, choose the best lambda value from the following set: {0.001, 0.003, 0.01, 0.03, 0.1,0.3,1,3,10,33}.\n",
    "\n",
    "To choose the best hyperparameter (alpha/lambda) value, you have to do the following:\n",
    "* For each value of hyperparameter, perform 100 random splits of training data into training and validation data. \n",
    "* Find the average validation accuracy for each 100 train/validate pairs.\n",
    "The best hyperparameter will be the one that gives maximum validation accuracy. Use the best alpha and lambda parameter to re-train your final L1 and L2 regularized model. Evaluate the prediction performance on the test data and report the following:\n",
    "* Precision\n",
    "* Accuracy\n",
    "* The top 5 features selected in decreasing order of feature weights.\n",
    "* Confusion matrix\n",
    "\n",
    "Finally, discuss if there is any sign of underfitting or overfitting with appropriate reasoning.\n",
    "\n",
    "### Part-2 (Multiclass Classification):\n",
    "\n",
    "For this experiment, we will use a small subset of [MNIST dataset for handwritten digits](http://yann.lecun.com/exdb/mnist/). This dataset has no missing data. You will have to implement one-versus-rest scheme to perform multi-class classification using a binary classifier based on L1 regularized logistic regression. \n",
    "\n",
    "**2.1 Read and understand the data, create a default One-vs-Rest Classifier **\n",
    "\n",
    "1- Use the data from the file reduced_mnist.csv in the data directory. Begin by reading the data. Print the following information:\n",
    "\n",
    "* Number of data points\n",
    "* Total number of features\n",
    "* Unique labels in the data\n",
    "\n",
    "2- Split the data into 70% training data and 30% test data. Fit a One-vs-Rest Classifier (which uses Logistic regression classifier with alpha=1) on training data, and report accuracy, precision, recall on testing data.\n",
    "\n",
    "**2.2 Choosing the best hyper-parameter **\n",
    "\n",
    "1- As in section **1.3** above, now create 10 random splits of training data into training and validation data. Choose the best value of alpha from the following set: {0.1, 1, 3, 10, 33, 100, 333, 1000, 3333, 10000, 33333}.\n",
    "To choose the best alpha hyperparameter value, you have to do the following:\n",
    "\n",
    "* For each value of hyperparameter, perform 10 random splits of training data into training and validation data as said above. \n",
    "* For each value of hyperparameter, use its 10 random splits and find the average training and validation accuracy.\n",
    "* On a graph, plot both the average training accuracy (in red) and average validation accuracy (in blue) w.r.t. each hyperparameter setting. Comment on this graph by identifying regions of overfitting and underfitting. \n",
    "* Print the best value of alpha hyperparameter.\n",
    "\n",
    "2- Evaluate the prediction performance on test data and report the following:\n",
    "* Total number of non-zero features in the final model.\n",
    "* The confusion matrix\n",
    "* Precision, recall and accuracy for each class.\n",
    "\n",
    "Finally, discuss if there is any sign of underfitting or overfitting with appropriate reasoning.\n",
    "\n",
    "## References\n",
    "* [Finding missing values](https://chartio.com/resources/tutorials/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe)\n",
    "* [Titanic Problem](http://nbviewer.jupyter.org/github/agconti/kaggle-titanic/blob/master/Titanic.ipynb)\n",
    "* [Numpy: Sorting and Searching](http://docs.scipy.org/doc/numpy/reference/routines.sort.html)\n",
    "* [Multiclass Classification](http://scikit-learn.org/stable/modules/multiclass.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1 (Binary Classification) Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Here we are going to import libraries that are to be used for executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as num                                                   #It is used for arrays, calculations and matrices\n",
    "import pandas as pd                                                   #It is used for reading and writing the csv files\n",
    "from sklearn.preprocessing import scale                               #It is used for normalizing the data\n",
    "from sklearn.linear_model import LogisticRegression                   #It is used for implememnting the L1 and L2 regualrizations\n",
    "from sklearn.metrics import recall_score                              #It is used for calculating the recall score\n",
    "from sklearn.metrics import f1_score                                  #It is used for calculating the f1 score\n",
    "from sklearn.metrics import confusion_matrix                          #It is used for calculating the confusion matrix\n",
    "from sklearn.metrics import accuracy_score                            #It is used for calculating the accuracy score\n",
    "from sklearn.metrics import precision_score                           #It is used for calculating the precision score\n",
    "from sklearn.cross_validation import train_test_split                 #It is used for splitting the data in 2 parts\n",
    "import matplotlib.pyplot as plt                                       #It is used for plotting graphs and charts\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data Munging Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Reading Files\n",
    "\n",
    "Here we are going to read the training and test data files using pandas and print the number of features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading training file and storing in the traindata variable\n",
    "traindata = pd.read_csv(\"data/wisconsin_data/train_wbcd.csv\", delimiter=',')\n",
    "#print traindata\n",
    "\n",
    "#Reading test file and storing in the testdata variable\n",
    "testdata = pd.read_csv(\"data/wisconsin_data/test_wbcd.csv\", delimiter=',')\n",
    "#print testdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the type of traindata and testdata is Dataframe, i'm using iloc to retrieve the values from it. After selecting the columns for each dataset, len function is used to find the total features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in train data are : 30\n",
      "Number of features in test data are  : 30\n"
     ]
    }
   ],
   "source": [
    "#Selecting all rows and the selected columns for the features from the training data\n",
    "train_feature = traindata.iloc[:,2:32]\n",
    "print \"Number of features in train data are :\",len(train_feature.T)\n",
    "\n",
    "#Selecting all rows and the selected columns for the features from the test data\n",
    "test_feature = testdata.iloc[:,2:32]\n",
    "print \"Number of features in test data are  :\",len(test_feature.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Finding 1's and 0's\n",
    "\n",
    "Here, we have to find number of 1's and 0's for training and test data. For this, we are first checking if the value in the traindata/testdata is equal to 1/0 and then selecting the sum along each row and then doing the final sum for that label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0 in training set is :  36\n",
      "Number of 1 in training set is :  0\n",
      "Number of 0 in test set is     :  6\n",
      "Number of 1 in test set is     :  0\n"
     ]
    }
   ],
   "source": [
    "#Calculating the 1's and 0's in the training data\n",
    "print \"Number of 0 in training set is : \",(traindata == 0).astype(int).sum(axis=0).sum()\n",
    "print \"Number of 1 in training set is : \",(traindata == 1).astype(int).sum(axis=0).sum()\n",
    "\n",
    "#Calculating the 1's and 0's in the test data\n",
    "print \"Number of 0 in test set is     : \",(testdata == 0).astype(int).sum(axis=0).sum()\n",
    "print \"Number of 1 in test set is     : \",(testdata == 1).astype(int).sum(axis=0).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the number of 0's and 1's in training set, it is 36 and 0 respectively. We can see the overall ratio is very high. From this, we can interpret that the class distribution is <b>unbalanced</b>. For the class distributio to be balanced the ratio of 1's and 0's should be same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Finding missing values\n",
    "\n",
    "Here, we have to find the missing values in training and test data. We are using pandas to convert it to dataframe, so that we can use the isnull() function under pandas to find the null values. If we use sum() function once, it will list null values for each feature, so to find the entire sum of missing values we are summing up the values for each feature again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with missing entries in training set :  2\n",
      "Features with missing entries in test set     :  1\n"
     ]
    }
   ],
   "source": [
    "#Computing the missing entries in the training data\n",
    "traindata = pd.DataFrame(traindata)\n",
    "print \"Features with missing entries in training set : \",traindata.isnull().sum().sum()\n",
    "\n",
    "#Computing the missing entries in the test data\n",
    "testdata = pd.DataFrame(testdata)\n",
    "print \"Features with missing entries in test set     : \",testdata.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Filling missing values\n",
    "\n",
    "Here, we have to fill the missing values in training and test data. From the above code, we found out the missing values were missing in the feature f21. To fill the values, we can either use mean or median values but here median is used. We are selecting the entire feature column f21 and using the median to calculate the value. To fill the value, fillna function is used that will subsitute the median value in the missing data. In the output we can see now there are no missing values in the train and test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with missing entries in training set :  0\n",
      "Features with missing entries in test set :  0\n"
     ]
    }
   ],
   "source": [
    "train_feature [\"f21\"] =train_feature [\"f21\"].fillna(train_feature [\"f21\"].median() )\n",
    "print \"Features with missing entries in training set : \",pd.isnull(train_feature).sum().sum()\n",
    "\n",
    "#print train_feature [\"f21\"].median()\n",
    "test_feature [\"f21\"] =test_feature [\"f21\"].fillna(test_feature [\"f21\"].median() )\n",
    "\n",
    "print \"Features with missing entries in test set : \",pd.isnull(test_feature).sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Normalizing the dataset\n",
    "\n",
    "We are using the scale function to normalize the traindata and testdata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feature = scale(train_feature)\n",
    "#print Xnorm_train_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_feature = scale(test_feature)\n",
    "#print Xnorm_test_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Logistic Regression\n",
    "\n",
    "We are training our model using logistic regression models with L1 regularization and L2 regularization using alpha = 0.1 and lambda = 0.1. We have created the function modeltraining which accepts either alpha/lambda value and the regularization value and then we use the logisticregression function to compute our model. To compute the different scores we have passed the true labels and the predicted labels in the following functions:\n",
    "\n",
    "<b>Accuracy</b> - accuracy_score\n",
    "\n",
    "<b>Precision</b> - precision_score\n",
    "\n",
    "<b>Recall</b> - recall_score\n",
    "\n",
    "<b>F1</b> - f1_score\n",
    "\n",
    "<b>Confusion Matrix</b> - confusion_matrix\n",
    "\n",
    "As we can see the labels are not in binary format, so we have used average='weighted'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modeltraining(val,regularization):\n",
    "        c_val = 1/val                                       #calculating the c value\n",
    "        \n",
    "        #initialize the logisitc regression model.\n",
    "        my_model = LogisticRegression(C=c_val,penalty=regularization)\n",
    "\n",
    "        #Create the training/testing data and labels\n",
    "        Xtrain = num.array(train_feature[:,:])\n",
    "        ytrain = traindata.iloc[:,1]\n",
    "\n",
    "        Xtest = num.array(test_feature[:,:])\n",
    "        ytest = testdata.iloc[:,1]\n",
    "\n",
    "        #print Xtrain.shape\n",
    "        #print ytrain.shape\n",
    "        #print Xtest.shape\n",
    "        #print ytest.shape\n",
    "        \n",
    "        my_model.fit(Xtrain, ytrain.values)               #fitting the model with the training values\n",
    "        \n",
    "        predict_label = my_model.predict(Xtest)           #predicting the model values\n",
    "\n",
    "        #the original label is ytest\n",
    "        true_label = ytest.values\n",
    "\n",
    "        #print \"The true labels are       : {}\".format(true_label)\n",
    "        #print \"Model predicted labels are: {}\".format(predict_label)'''\n",
    "        \n",
    "        print \n",
    "        print \"Model accuracy is: {}%\".format(num.round(accuracy_score(true_label, predict_label)*100, decimals=2))\n",
    "        print\n",
    "        print \"Confusion Matrix is :\"\n",
    "        print confusion_matrix(true_label, predict_label, labels=[\"B\", \"M\"])\n",
    "        print\n",
    "        print \"Precision score is :\",precision_score(true_label, predict_label,labels=[\"B\",\"M\"],average='weighted')\n",
    "        print \"Recall score is: \",recall_score(true_label, predict_label,labels=[\"B\",\"M\"],average='weighted')\n",
    "        print \"F1-Score is :\",f1_score(true_label, predict_label,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training using <b>L1</b> regularization. In the output, we can see the accuracy accounts to 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model accuracy is: 90.0%\n",
      "\n",
      "Confusion Matrix is :\n",
      "[[13  1]\n",
      " [ 1  5]]\n",
      "\n",
      "Precision score is : 0.9\n",
      "Recall score is:  0.9\n",
      "F1-Score is : 0.9\n"
     ]
    }
   ],
   "source": [
    "#assigining the alpha value and calling the above function modeltraining with l1 regularization\n",
    "\n",
    "alpha = 0.1\n",
    "modeltraining(num.float(alpha),'l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training using <b>L2</b> regularization. In the output, we can see the accuracy accounts to 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model accuracy is: 90.0%\n",
      "\n",
      "Confusion Matrix is :\n",
      "[[13  1]\n",
      " [ 1  5]]\n",
      "\n",
      "Precision score is : 0.9\n",
      "Recall score is:  0.9\n",
      "F1-Score is : 0.9\n"
     ]
    }
   ],
   "source": [
    "#assigining the lambda value and calling the above function modeltraining with l2 regularization\n",
    "\n",
    "lambdaval = 0.1\n",
    "modeltraining(num.float(lambdaval),'l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Choosing the best hyper-parameter\n",
    "\n",
    "### a. Random Splits\n",
    "\n",
    "For choosing the best hyper parameter we have to perform 100 random splits for training data into training and validation data and compute the average accuracy for each pair. We have created the function besthyperparameter which accepts either alpha/lambda value and also regularization l1 and l2 respectively. We have added the labels to the training features using the column stack under the numpy library and then split the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def besthyperparameter(val,regularization):\n",
    "    trials=100                                                      #To run loop 100 times\n",
    "    c_val = 1/val                                                   #calculating the c value\n",
    "    my_model = LogisticRegression(C=c_val,penalty=regularization)   #initializing the model \n",
    "    model_accs  = num.zeros(trials)                                 #storing model accuracy\n",
    "    \n",
    "    for i in range(0,trials):\n",
    "        \n",
    "        #Spliting the training data into training and validation data\n",
    "        #train_feature only contains features and not the labels\n",
    "        #using column stack from numpy library for appending the labels columns to the features column\n",
    "        #using train_test_split function to split the training data\n",
    "        \n",
    "        Dtrain, Dval = train_test_split((num.column_stack((train_feature,(traindata.iloc[:,1])))), test_size=0.3)\n",
    "        \n",
    "        #Dtrain contains the training data\n",
    "        Xtrain = num.array(Dtrain[:, 0:30])\n",
    "        ytrain = num.array(Dtrain[:,30])\n",
    "\n",
    "        #Dval contains the validation data\n",
    "        Xval = num.array(Dval[:, 0:30])\n",
    "        yval = num.array(Dval[:,30])\n",
    "        \n",
    "        #fitting the model with the training values\n",
    "        my_model.fit(Xtrain,ytrain)\n",
    "        \n",
    "        ypredicts = my_model.predict(Xval)                         #prediciting the labels using the validation data\n",
    "        ytruelabels = yval                                         #calculating the true lables\n",
    "        model_accs[i]  = accuracy_score(ytruelabels, ypredicts)    #For each iteration storing the accuracy value\n",
    "        \n",
    "        #To compute average accuracy over 100 random splits we are taking the mean of all the values\n",
    "    return num.mean(model_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are calling the above defined functions with the set of alpha values and l1 regularization. We are computing the average accuracy for each alpha value and finding out the best alpha value which gives the maximum accuracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score when alpha is 0.1 using L1 model is : 0.987666666667\n",
      "Accuracy Score when alpha is 1 using L1 model is : 0.985333333333\n",
      "Accuracy Score when alpha is 3 using L1 model is : 0.972333333333\n",
      "Accuracy Score when alpha is 10 using L1 model is : 0.938666666667\n",
      "Accuracy Score when alpha is 33 using L1 model is : 0.58\n",
      "Accuracy Score when alpha is 100 using L1 model is : 0.579666666667\n",
      "Accuracy Score when alpha is 333 using L1 model is : 0.582\n",
      "Accuracy Score when alpha is 1000 using L1 model is : 0.570666666667\n",
      "Accuracy Score when alpha is 3333 using L1 model is : 0.582\n",
      "Accuracy Score when alpha is 10000 using L1 model is : 0.571333333333\n",
      "Accuracy Score when alpha is 33333 using L1 model is : 0.585333333333\n",
      "Best Alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "alpha_vals = [0.1,1,3,10,33,100,333,1000, 3333, 10000, 33333]                 #Array of alpha values\n",
    "l1_acc = num.zeros(len(alpha_vals))                                           #Creating an array of equal length\n",
    "index=0\n",
    "\n",
    "for l in alpha_vals:\n",
    "    l1_acc[index] = besthyperparameter(num.float(l),'l1')                     #Calling function with each alpha value\n",
    "    index+=1\n",
    "\n",
    "index = 0\n",
    "for acc_val in alpha_vals:\n",
    "    print \"Accuracy Score when alpha is {} using L1 model is : {}\".format(alpha_vals[index],l1_acc[index])\n",
    "    index+=1\n",
    "    \n",
    "max_index_l1  = num.argmax(l1_acc)                                            #Using argmax to find the index with max value\n",
    "best_alpha = alpha_vals[max_index_l1]                                         #Passing the max value to retrieve the alpha value\n",
    "    \n",
    "print \"Best Alpha: {}\".format(best_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are calling the above defined functions with the set of lambda values and l2 regularization. We are computing the average accuracy for each lambda value and finding out the best lambda value which gives the maximum accuracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score when lambda is 0.001 using L2 model is : 0.988\n",
      "Accuracy Score when lambda is 0.003 using L2 model is : 0.987333333333\n",
      "Accuracy Score when lambda is 0.01 using L2 model is : 0.988\n",
      "Accuracy Score when lambda is 0.03 using L2 model is : 0.993\n",
      "Accuracy Score when lambda is 0.1 using L2 model is : 0.994333333333\n",
      "Accuracy Score when lambda is 0.3 using L2 model is : 0.992333333333\n",
      "Accuracy Score when lambda is 1 using L2 model is : 0.994333333333\n",
      "Accuracy Score when lambda is 3 using L2 model is : 0.992333333333\n",
      "Accuracy Score when lambda is 10 using L2 model is : 0.991333333333\n",
      "Accuracy Score when lambda is 33 using L2 model is : 0.976333333333\n",
      "Best Lambda: 0.1\n"
     ]
    }
   ],
   "source": [
    "lambda_vals = [0.001, 0.003, 0.01, 0.03, 0.1,0.3,1,3,10,33]                  #Array of lambda values\n",
    "l2_acc = num.zeros(len(lambda_vals))                                         #Creating an array of equal length\n",
    "index=0\n",
    "\n",
    "for l in lambda_vals:\n",
    "    l2_acc[index] = besthyperparameter(num.float(l),'l2')                    #Calling function with each lambda value\n",
    "    index+=1\n",
    "\n",
    "index = 0\n",
    "for acc_val in lambda_vals:\n",
    "    print \"Accuracy Score when lambda is {} using L2 model is : {}\".format(lambda_vals[index],l2_acc[index])\n",
    "    index+=1\n",
    "    \n",
    "max_index_l2  = num.argmax(l2_acc)                                          #Using argmax to find the index with max value\n",
    "best_lambda = lambda_vals[max_index_l2]                                     #Passing the max value to retrieve the lambda value\n",
    "    \n",
    "print \"Best Lambda: {}\".format(best_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Retraining Model\n",
    "\n",
    "Now we have to retrain our model using the best alpha and lambda values using l1 and l2 regularization and predict the performance on the test data. We need to also evaluate the accuracy, precision, recall, f1-score, confusion matrix and the top five weights in the decreasing order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrainModel(val,regularization):\n",
    "    \n",
    "    c_val=num.float(1/val)                                                    #calculating the c value\n",
    "    my_model = LogisticRegression(C=c_val,penalty=regularization)             #initializing the model\n",
    "\n",
    "    #Create the training/testing data and labels\n",
    "    Xtrain = num.array(train_feature[:,:])\n",
    "    ytrain = traindata.iloc[:,1]\n",
    "\n",
    "    Xtest = num.array(test_feature[:,:])\n",
    "    ytest = testdata.iloc[:,1]\n",
    "    \n",
    "    my_model.fit(Xtrain, ytrain.values)                                       #fitting the model with the training values\n",
    "    \n",
    "    predict_label = my_model.predict(Xtest)                                   #predicting the model values\n",
    "\n",
    "    true_label = ytest.values                                                 #the original label is ytest\n",
    "\n",
    "    model_weights   = my_model.coef_                                          #calculating the coefficients\n",
    "    weights = ((num.argsort(model_weights))[0])[::-1]                         #sorting the model weights in descending order\n",
    "    \n",
    "    #print \"The true labels are       : {}\".format(true_label)\n",
    "    #print\n",
    "    #print \"Model predicted labels are: {}\".format(predict_label)\n",
    "    #print\n",
    "    \n",
    "    print accuracy_score(true_label, predict_label)\n",
    "    print \"Model accuracy is: {}%\".format(num.round(accuracy_score(true_label, predict_label)*100, decimals=2))\n",
    "    print\n",
    "    print \"Confusion Matrix is :\"\n",
    "    print confusion_matrix(true_label, predict_label, labels=[\"B\", \"M\"])\n",
    "    print\n",
    "    print \"Precision score is :\",precision_score(true_label, predict_label,labels=[\"B\",\"M\"],average='weighted')\n",
    "    print \"Recall score is: \",recall_score(true_label, predict_label,labels=[\"B\",\"M\"],average='weighted')\n",
    "    print \"F1-Score is :\",f1_score(true_label, predict_label,average='weighted')\n",
    "    print\n",
    "    print \"Top 5 features selected in decreasing order of feature weights are:\"\n",
    "    print weights[0:5]+1                                                     #taking top 5 weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the <b>best alpha value</b> to call the retrainModel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "Model accuracy is: 90.0%\n",
      "\n",
      "Confusion Matrix is :\n",
      "[[13  1]\n",
      " [ 1  5]]\n",
      "\n",
      "Precision score is : 0.9\n",
      "Recall score is:  0.9\n",
      "F1-Score is : 0.9\n",
      "\n",
      "Top 5 features selected in decreasing order of feature weights are:\n",
      "[23 22 25 21  4]\n"
     ]
    }
   ],
   "source": [
    "#retraining the model with best alpha value and l1 regularization\n",
    "retrainModel(best_alpha,'l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the <b>best lambda value</b> to call the retrainModel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "Model accuracy is: 90.0%\n",
      "\n",
      "Confusion Matrix is :\n",
      "[[13  1]\n",
      " [ 1  5]]\n",
      "\n",
      "Precision score is : 0.9\n",
      "Recall score is:  0.9\n",
      "F1-Score is : 0.9\n",
      "\n",
      "Top 5 features selected in decreasing order of feature weights are:\n",
      "[22  2 25 21 23]\n"
     ]
    }
   ],
   "source": [
    "#retraining the model with best lambda value and l2 regularization\n",
    "retrainModel(best_lambda,'l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Underfitting/Overfitting</b>\n",
    "\n",
    "A model that can not model the training model or cannot generalize the data is overfitting and it usually gives low performance on data. Whereas if a model trains too well it is overfitting, but there are chances that this model may react to noise and hence cause a negative impact. \n",
    "\n",
    "When we used different values of alpha for L1 and lambda for L2 regularizations, we saw that training accuracy went down drastically for L1 when alpha value was changed from 10 to 33. If the performance goes down, it means the model is overfitting and learning the irrelevant noise and details. We need to find the best fit for the data before it starts giving the error. But in case of L2 regularization, there was just a slight variation for the accuracy. It was basically reacting to each value of lambda and may be a good fit for the data.\n",
    "\n",
    "When we retrained our model with the best alpha and lambda values, accuracy was still the same. along with the precision, recall and f1 score. Hence the best value is the best fit for the data as it si not adhering to any noise. We can interpret that the top 5 features are different when using l1 reglarization and l2 regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part -2 (Multiclass Classification) Solutions\n",
    "\n",
    "## 2.1 One-vs-Rest Classifier\n",
    "\n",
    "### 1. Reading files and Printing data\n",
    "\n",
    "Here we are reading the data file using pandas and finding the following:\n",
    "\n",
    "a. Number of data points - By using the len function\n",
    "\n",
    "b. Total number of features - By taking transpose of data and finding the total features.\n",
    "\n",
    "c. Unique labels - By using unique function under numpy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points are : 2520\n",
      "Total Number of features in data are : 784\n",
      "Unique labels in the data are : [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "#Reading the file\n",
    "data = pd.read_csv(\"data/mnist/reduced_mnist.csv\", delimiter=',')\n",
    "print \"Number of data points are :\", len(data)\n",
    "\n",
    "#Computing number of features in the data\n",
    "data_feature = data.iloc[:,1:]\n",
    "print \"Total Number of features in data are :\",len(data_feature.T)\n",
    "\n",
    "#Computing unique labels in the data\n",
    "unique_labels = num.unique(data.iloc[:,0])\n",
    "print \"Unique labels in the data are :\", unique_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fitting One-vs-Rest Classifier\n",
    "\n",
    "In this part, we have to split the data. We can directly take the 70% and 30% of data, but here we are randomly spliting the data into 70% training and 30% test data as it is not mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1781\n",
      "739\n"
     ]
    }
   ],
   "source": [
    "#Randomly selecting the data by spliting it into 70% and 30%\n",
    "mask = num.random.rand(len(data)) < 0.7\n",
    "\n",
    "#taking the 70% data\n",
    "train = data[mask]\n",
    "\n",
    "#taking the 30% data\n",
    "test = data[~mask]\n",
    "\n",
    "print len(train)                                    #length of the training data\n",
    "print len(test)                                     #Length of the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created the function onevsrestClassifier which takes the alpha value and trains the model using l1 regularization and then computes the accuracy, precision, confusion matrix and recall score. By default, logistic regression uses the one vs rest classifier as it sets the multi_class='ovr'. If you want to write it, you can use <b>multi_class='ovr'</b> inside the logisticregression function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onevsrestClassifier(val):\n",
    "    c_val=num.float(1/val)                                               #calculating the c value\n",
    "    my_model = LogisticRegression(C=c_val,penalty='l1')                  #initializing the logistic regression model\n",
    "\n",
    "    #Create the training/testing data and labels\n",
    "    Xtrain = train.iloc[:,:]\n",
    "    ytrain = train.iloc[:,0]\n",
    "\n",
    "    Xtest = test.iloc[:,:]\n",
    "    ytest = test.iloc[:,0]\n",
    "    \n",
    "    my_model.fit(Xtrain.values, ytrain.values)                           #fitting the model with the training values\n",
    "    \n",
    "    predict_label = my_model.predict(Xtest)                              #predicting the model with the test value\n",
    "\n",
    "    true_label = ytest.values                                            #the original label is ytest\n",
    "    \n",
    "    print accuracy_score(true_label, predict_label)\n",
    "    print \"Model accuracy is: {}%\".format(num.round(accuracy_score(true_label, predict_label)*100, decimals=2))\n",
    "    print\n",
    "    print \"Confusion Matrix is :\"\n",
    "    print confusion_matrix(true_label, predict_label)\n",
    "    print\n",
    "    print \"Precision score is :\",precision_score(true_label, predict_label,average='weighted')\n",
    "    print \"Recall score is: \",recall_score(true_label, predict_label,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the above defined function by passing alpha value as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.845737483085\n",
      "Model accuracy is: 84.57%\n",
      "\n",
      "Confusion Matrix is :\n",
      "[[62  1  1  0  0  0  1  1  1  0]\n",
      " [ 0 75  1  1  0  1  0  0  0  0]\n",
      " [ 1  3 65  3  2  1  2  0  3  0]\n",
      " [ 1  0  8 64  0  6  1  0  0  1]\n",
      " [ 1  0  0  0 63  0  1  1  5  7]\n",
      " [ 4  0  3  5  0 58  1  0  5  2]\n",
      " [ 2  0  0  0  0  0 59  0  0  0]\n",
      " [ 0  1  2  0  2  0  0 65  3  3]\n",
      " [ 0  3  2  1  1  3  0  0 50  2]\n",
      " [ 2  0  2  0  2  2  0  3  3 64]]\n",
      "\n",
      "Precision score is : 0.848010027283\n",
      "Recall score is:  0.845737483085\n"
     ]
    }
   ],
   "source": [
    "#Using alpha value as 1 we are calling the onevsrestClassifier function\n",
    "onevsrestClassifier(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Choosing the best hyper-parameter\n",
    "\n",
    "### 1. Random Splits for Training\n",
    "\n",
    "Here we are taking 10 random splits for the training data into training and validation, and finding the average accuracy for each pair. We are spliting our training data into Dtrain and Dval by giving the test size as 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choosebestvalhyperparameter(val):\n",
    "    trials=10                                                              #running the loop for 10 times\n",
    "    c_val = 1/val                                                          #calculating the c value\n",
    "    my_model = LogisticRegression(C=c_val,penalty='l1')                    #initalizing the logistic regression model with l1\n",
    "    model_accs  = num.zeros(trials)                                        #storing model accuracy\n",
    "    \n",
    "    for i in range(0,trials):\n",
    "        Dtrain, Dval = train_test_split(train.iloc[:,:], test_size=0.3)    #Spliting the training into training and validation\n",
    "    \n",
    "        Xtrain = Dtrain.iloc[:, 1:]\n",
    "        ytrain = Dtrain.iloc[:,0]\n",
    "\n",
    "        Xval = Dval.iloc[:, 1:]\n",
    "        yval = Dval.iloc[:,0]\n",
    "        \n",
    "        #print Xtrain.shape\n",
    "        #print ytrain.shape\n",
    "        #print Xval.shape\n",
    "        #print yval.shape\n",
    "        \n",
    "        my_model.fit(Xtrain,ytrain)                                        #fitting the model with the training values\n",
    "        \n",
    "        ypredicts = my_model.predict(Xval)                                 #predicting the values using val values\n",
    "        ytruelabels = yval                                                 #assiging the values to true labels\n",
    "        model_accs[i]  = accuracy_score(ytruelabels, ypredicts)            #storing accuracy for each run\n",
    "        \n",
    "        #To find the average value of accuracy, we are taking the mean of all the values\n",
    "    return num.mean(model_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are calling the above defined functions with the set of alpha values and l1 regularization. We are computing the average accuracy and finding out the best alpha value which gives the maximum accuracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score when alpha is 0.1 using L1 model is : 0.842990654206\n",
      "Accuracy Score when alpha is 1 using L1 model is : 0.840186915888\n",
      "Accuracy Score when alpha is 3 using L1 model is : 0.835700934579\n",
      "Accuracy Score when alpha is 10 using L1 model is : 0.841121495327\n",
      "Accuracy Score when alpha is 33 using L1 model is : 0.84953271028\n",
      "Accuracy Score when alpha is 100 using L1 model is : 0.851401869159\n",
      "Accuracy Score when alpha is 333 using L1 model is : 0.862803738318\n",
      "Accuracy Score when alpha is 1000 using L1 model is : 0.851962616822\n",
      "Accuracy Score when alpha is 3333 using L1 model is : 0.800934579439\n",
      "Accuracy Score when alpha is 10000 using L1 model is : 0.711962616822\n",
      "Accuracy Score when alpha is 33333 using L1 model is : 0.556074766355\n",
      "Best Alpha: 333\n"
     ]
    }
   ],
   "source": [
    "alpha_vals = [0.1, 1, 3, 10, 33, 100, 333, 1000, 3333, 10000, 33333]             #Creating an array of alpha values\n",
    "\n",
    "l1_val_acc = num.zeros(len(alpha_vals))                                          #creating an array of equal length\n",
    "index=0\n",
    "\n",
    "for l in alpha_vals:\n",
    "    l1_val_acc[index] = choosebestvalhyperparameter(num.float(l))                #calling the function with each alpha value\n",
    "    index+=1\n",
    "\n",
    "index = 0\n",
    "for acc_val in alpha_vals:\n",
    "    print \"Accuracy Score when alpha is {} using L1 model is : {}\".format(alpha_vals[index],l1_val_acc[index])\n",
    "    index+=1\n",
    "    \n",
    "max_index_l1_val  = num.argmax(l1_val_acc)                                      #Finding the index with the max accuracy value\n",
    "best_val_alpha = alpha_vals[max_index_l1_val]                                   #Finding the best alpha value\n",
    "    \n",
    "print \"Best Alpha: {}\".format(best_val_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Splits for Data\n",
    "\n",
    "Here we are taking 10 random splits for the data into training and testing, and finding the average accuracy for each pair. We are spliting our original data into Dtrain and Dval by giving the test size as 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choosebesttrainhyperparameter(val):\n",
    "    trials=10                                                               #Running the loop 10 times\n",
    "    c_val = 1/val                                                           #Calculating the c value\n",
    "    my_model = LogisticRegression(C=c_val,penalty='l1')                     #Initializing the logistic regression model with l1\n",
    "    model_accs  = num.zeros(trials)                                         #storing model accuracy\n",
    "    \n",
    "    for i in range(0,trials):\n",
    "        Dtrain, Dval = train_test_split(data.iloc[:,:], test_size=0.3)      #Splitting the data into training and test\n",
    "    \n",
    "        Xtrain = Dtrain.iloc[:, 1:]\n",
    "        ytrain = Dtrain.iloc[:,0]\n",
    "\n",
    "        Xtest = Dval.iloc[:, 1:]\n",
    "        ytest = Dval.iloc[:,0]\n",
    "        \n",
    "        #print Xtrain.shape\n",
    "        #print ytrain.shape\n",
    "        #print Xval.shape\n",
    "        #print yval.shape\n",
    "        \n",
    "        my_model.fit(Xtrain,ytrain)                                         #fitting the model with the training values\n",
    "        \n",
    "        ypredicts = my_model.predict(Xtest)                                 #prediciting the labels with the test values\n",
    "        ytruelabels = ytest                                                 #assiging the true labels\n",
    "        model_accs[i]  = accuracy_score(ytruelabels, ypredicts)             #storing accuracy for each run\n",
    "        \n",
    "        #To find the average value of accuracy, we are taking the mean of all the values\n",
    "        \n",
    "    return num.mean(model_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are calling the above defined functions with the set of alpha values and l1 regularization. We are computing the average accuracy and finding out the best alpha value which gives the maximum accuracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score when alpha is 0.1 using L1 model is : 0.834391534392\n",
      "Accuracy Score when alpha is 1 using L1 model is : 0.836772486772\n",
      "Accuracy Score when alpha is 3 using L1 model is : 0.833333333333\n",
      "Accuracy Score when alpha is 10 using L1 model is : 0.835582010582\n",
      "Accuracy Score when alpha is 33 using L1 model is : 0.845899470899\n",
      "Accuracy Score when alpha is 100 using L1 model is : 0.859788359788\n",
      "Accuracy Score when alpha is 333 using L1 model is : 0.870502645503\n",
      "Accuracy Score when alpha is 1000 using L1 model is : 0.860846560847\n",
      "Accuracy Score when alpha is 3333 using L1 model is : 0.826984126984\n",
      "Accuracy Score when alpha is 10000 using L1 model is : 0.738888888889\n",
      "Accuracy Score when alpha is 33333 using L1 model is : 0.605158730159\n",
      "Best Alpha: 333\n"
     ]
    }
   ],
   "source": [
    "alpha_vals = [0.1, 1, 3, 10, 33, 100, 333, 1000, 3333, 10000, 33333]          #Creating an array of alpha values\n",
    "\n",
    "l1_train_acc = num.zeros(len(alpha_vals))                                     #creating an array of equal length\n",
    "index=0\n",
    "\n",
    "for l in alpha_vals:\n",
    "    l1_train_acc[index] = choosebesttrainhyperparameter(num.float(l))         #calling the function with each alpha value\n",
    "    index+=1\n",
    "\n",
    "index = 0\n",
    "for acc_val in alpha_vals:\n",
    "    print \"Accuracy Score when alpha is {} using L1 model is : {}\".format(alpha_vals[index],l1_train_acc[index])\n",
    "    index+=1\n",
    "    \n",
    "max_index_l1_train  = num.argmax(l1_train_acc)                                #Finding the index with the max accuracy value\n",
    "best_train_alpha = alpha_vals[max_index_l1_train]                             #Finding the best alpha value\n",
    "    \n",
    "print \"Best Alpha: {}\".format(best_train_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Plotting Average Training Accuracy/ Validation Accuracy\n",
    "\n",
    "Here we are going to plot the graph for the average training accuracy versus average validation accuracy. From the graph, we can see both the training and validation accuracy are producing the best accuracy for the alpha value 333. \n",
    "\n",
    "Underfitting can usually be detected by observing low accuracy on training set, but in this case it is reacting to each value of alpha. It is difficult to tell if it is underfitting. \n",
    "\n",
    "Overfitting can usually be detected by if there is great mismatch between accuracies on train and validation sets. Here, we can see there is not a great mismatch between the values, there is just a slight variation.It's normal to have validation set accuracy slightly lower than train set accuracy, which can be seen after the alpha value is increased after 333. IT is difficult to discuss if it is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1092b908>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAESCAYAAAAG+ZUXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd2cd978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the accuracy curve\n",
    "plt.plot(range(0,len(alpha_vals)), l1_val_acc, color='b', label='Validation Acuracy')\n",
    "plt.plot(range(0,len(alpha_vals)), l1_train_acc, color='r', label='Training Accuracy')\n",
    "\n",
    "#replace the x-axis labels with penalty values\n",
    "plt.xticks(range(0,len(alpha_vals)), alpha_vals, rotation='vertical') \n",
    "\n",
    "#Highlight the best values of alpha for training and validation accuracy\n",
    "plt.plot((max_index_l1_val, max_index_l1_val), (0, l1_val_acc[max_index_l1_val]), ls='dotted', color='b')\n",
    "plt.plot((max_index_l1_train, max_index_l1_train), (0, l1_train_acc[max_index_l1_train]), ls='dotted', color='r')\n",
    "\n",
    "#Set the y-axis from 0 to 1.0\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0, 1.0])\n",
    "\n",
    "plt.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluating Performance on Test Data\n",
    "\n",
    "Here, we are going to retrain our model using the best alpha value and compute the following:\n",
    "\n",
    "a. Number of non-zero features\n",
    "\n",
    "b. Confusion Matrix\n",
    "\n",
    "c. Precision, Accuracy and Recall for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrainfinalModel(val):\n",
    "    \n",
    "    c_val=num.float(1/val)                                             #Calculating the c value\n",
    "    my_model = LogisticRegression(C=c_val,penalty='l1')                #initializing the logistic regression model with l1\n",
    "\n",
    "    #Create the training/testing data and labels\n",
    "    Xtrain = train.iloc[:,:]\n",
    "    ytrain = train.iloc[:,0]\n",
    "\n",
    "    Xtest = test.iloc[:,:]\n",
    "    ytest = test.iloc[:,0]\n",
    "    \n",
    "    my_model.fit(Xtrain.values, ytrain.values)                         #fitting the model with the training values\n",
    "    \n",
    "    predict_label = my_model.predict(Xtest)                            #prediciting the values with the test values\n",
    "\n",
    "    true_label = ytest.values                                          #the original label is ytest\n",
    "\n",
    "    print \"Number of non-zero feautres are: \", num.count_nonzero(my_model.coef_)    #Computing number of non zero features\n",
    "    print\n",
    "    print \"Confusion Matrix is :\"\n",
    "    print confusion_matrix(true_label, predict_label)\n",
    "    \n",
    "    cm = confusion_matrix(true_label, predict_label)                    #Computing the confusion matrix\n",
    "    TP = num.diag(cm)                                                   #Computing the True Positive\n",
    "    FP = cm.sum(axis=0) - num.diag(cm)                                  #Computing the False Positive\n",
    "    FN = cm.sum(axis=1) - num.diag(cm)                                  #Computing the False Negative\n",
    "    TN = cm.sum() - (FP + FN + TP)                                      #Computing the True Negative\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    #Creating an empty array\n",
    "    precision = []                                                      \n",
    "    recall = []\n",
    "    accuracy = []\n",
    "    \n",
    "    while i in range(0,10):\n",
    "        precision.append(float(TP[i])/float(TP[i] + FP[i]))                       #Computing the precision for each class\n",
    "        recall.append(float(TP[i])/float(TP[i]+FN[i]))                            #Computing the recall for each class\n",
    "        accuracy.append(float(TP[i] + TN[i])/float(TP[i]+TN[i]+FP[i]+FN[i])*100)  #Computing the accuracy for each class\n",
    "        i+=1\n",
    "    \n",
    "    i=0\n",
    "    print\n",
    "    print \"Labels                    Precision                        Recall                               Accuracy\"\n",
    "    while i in range(0,10):\n",
    "           print \"{}                        {}                   {}                        {}\" .format(i,precision[i],recall[i],accuracy[i])\n",
    "           print\n",
    "           i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the above defined function using the best alpha value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-zero feautres are:  4292\n",
      "\n",
      "Confusion Matrix is :\n",
      "[[64  1  1  0  0  0  0  0  1  0]\n",
      " [ 0 77  1  0  0  0  0  0  0  0]\n",
      " [ 1  1 66  0  1  1  4  0  6  0]\n",
      " [ 3  0  6 65  0  5  1  0  0  1]\n",
      " [ 1  0  0  0 66  0  0  1  4  6]\n",
      " [ 2  0  1  2  0 61  3  2  7  0]\n",
      " [ 3  0  0  0  0  0 57  0  0  1]\n",
      " [ 0  1  1  0  2  0  0 67  2  3]\n",
      " [ 0  3  2  0  1  4  0  0 50  2]\n",
      " [ 2  0  1  0  2  1  0  3  5 64]]\n",
      "\n",
      "Labels                    Precision                        Recall                               Accuracy\n",
      "0                        0.842105263158                   0.955223880597                        97.9702300406\n",
      "\n",
      "1                        0.927710843373                   0.987179487179                        99.0527740189\n",
      "\n",
      "2                        0.835443037975                   0.825                        96.3464140731\n",
      "\n",
      "3                        0.970149253731                   0.802469135802                        97.5642760487\n",
      "\n",
      "4                        0.916666666667                   0.846153846154                        97.5642760487\n",
      "\n",
      "5                        0.847222222222                   0.782051282051                        96.2110960758\n",
      "\n",
      "6                        0.876923076923                   0.934426229508                        98.3761840325\n",
      "\n",
      "7                        0.917808219178                   0.881578947368                        97.9702300406\n",
      "\n",
      "8                        0.666666666667                   0.806451612903                        94.9932341001\n",
      "\n",
      "9                        0.831168831169                   0.820512820513                        96.3464140731\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Retraining the final model with the best alpha value\n",
    "retrainfinalModel(num.float(best_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After retraining our model with the best alpha value, we can see the accuracy, precision and recall for each class label. As the precision and accuracy vary for each label, we can interpret that the model may be reacting to the noise and hence the data may be overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
